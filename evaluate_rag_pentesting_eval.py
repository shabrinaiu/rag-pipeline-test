# Evaluate RAG Pipeline on Pentesting-Eval Dataset using DeepEval
from deepeval import evaluate
from deepeval.metrics import AnswerRelevancyMetric, HallucinationMetric
from deepeval.test_case import LLMTestCase
from datasets import load_dataset
from embeddings.chroma import ChromaVectorStore
from generators.gemma_generator import GemmaGenerator
from pipelines import RagPipeline
import json

# Load pentesting questions
def load_pentesting_eval_questions():
    dataset = load_dataset("preemware/pentesting-eval", split="train")
    questions = []
    for item in dataset:
        questions.append({
            "question": item["question"],
            "choices": item["choices"],
            "answer": item["answer"],
            "explanation": item.get("explanation", "")
        })
    return questions

# Prepare pipeline
vector_store = ChromaVectorStore()
dataset = load_dataset()
vector_store.add_documents(dataset)
rag_pipeline = RagPipeline(
    retriever=vector_store,
    generator=GemmaGenerator(model_name="google/gemma-2-8b"),
)

# Load pentesting questions
test_questions = load_pentesting_eval_questions()

# Prepare DeepEval test cases (limit to 10 for demo)
test_cases = []
for q in test_questions[:10]:
    user_query = q["question"]
    expected_output = q["answer"]
    # Use first 3 docs as context for demo; in real use, retrieve relevant docs
    retrieval_context = [doc.page_content for doc in dataset[:3]]
    actual_output = rag_pipeline.run(user_query)
    test_cases.append(
        LLMTestCase(
            input=user_query,
            actual_output=actual_output,
            expected_output=expected_output,
            retrieval_context=retrieval_context,
        )
    )

metrics = [
    AnswerRelevancyMetric(threshold=0.7),
    HallucinationMetric(threshold=0.3),
]

evaluate(test_cases, metrics)
