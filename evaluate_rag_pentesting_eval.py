# Evaluate RAG Pipeline on Pentesting-Eval Dataset using DeepEval
from deepeval import evaluate
from deepeval.metrics import AnswerRelevancyMetric, HallucinationMetric
from deepeval.test_case import LLMTestCase
from datasets import load_dataset
from preprocessors import load_dataset as load_local_dataset
from embeddings.chroma import ChromaVectorStore
from generators.gemma_generator import GemmaGenerator
from pipelines import RagPipeline
from utils.config import default_llm_model

# Load pentesting questions
def load_pentesting_eval_questions():
    dataset = load_dataset("preemware/pentesting-eval", split="train", keep_in_memory=True)
    questions = []
    for item in dataset:
        questions.append({
            "question": item["question"],
            "answer": item["choices"][item["answer"]],
            "explanation": item.get("explanation", "")
        })
    return questions

# Prepare pipeline
vector_store = ChromaVectorStore()
dataset = load_local_dataset()
vector_store.add_documents(dataset)
rag_pipeline = RagPipeline(
    retriever=vector_store,
    generator=GemmaGenerator(model_name=default_llm_model),
)

# Load pentesting questions
test_questions = load_pentesting_eval_questions()

# Prepare DeepEval test cases (limit to 10 for demo)
test_cases = []
for q in test_questions[:10]:
    user_query = q["question"]
    expected_output = q["answer"]
    retrieval_context = [doc.page_content for doc in vector_store.search_documents(user_query)]
    actual_output = rag_pipeline.run(user_query)
    test_cases.append(
        LLMTestCase(
            input=user_query,
            actual_output=actual_output,
            expected_output=expected_output,
            retrieval_context=retrieval_context,
        )
    )

metrics = [
    AnswerRelevancyMetric(threshold=0.7),
    HallucinationMetric(threshold=0.3),
]

result = evaluate(test_cases, metrics)
print(result)
